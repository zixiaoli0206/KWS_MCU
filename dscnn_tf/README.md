# Framework for MCU implementation of 12-class keyword spotting
Tensorflow framework to implement 12-class keyword spotting by both depthwise separable CNN (ds-cnn) and RNN (GRU & LSTM).

# Supported Dataset
* Google Speech Command Dataset Version 2

# Prerequisite
Install Miniconda
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod +x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
```

Create an environment using the following command:
```
conda create -n cochclass python=3.10 numpy matplotlib pandas tqdm h5py \
    scipy jupyter seaborn scikit-learn editdistance  \
    pytorch pydub soundfile torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
```

Activate the environment before running the script.
```
conda activate cochclass
```

Add some packages
```
pip install tensorflow
pip install serial
pip install playsound
```

Get the training feature:
The features are generated by rnn pytorch repo (uploaded separately). I leave the test features in the directory and 
store the training and validation data in polybox: https://polybox.ethz.ch/index.php/s/UsZ9YnOEOvCjvsC 
Please download them and put the files in the ./dataset/

# Run
Navigate to the code folder ./code and run experiments with the main.py file by specifying the target dataset. 
Run the code with --step to run the specific step. For example:

Training DS-CNN
```
python main.py --step ds_cnn_train
```
Quantization-aware training for DS-CNN
```
python main.py --step ds_cnn_qat
```
Evaluate DS-CNN
```
python main.py --step eval_ds_cnn
```
Transfer trained RNN in pytorch to tensorflow
```
python main.py --step rnn_transfer
```
Evaluate eval_rnn
```
python main.py --step eval_rnn
```
Evaluate the model size and count the MAC operations
```
python main.py --step model_size
```

# Run demo
The required c head files are stored in ./cfiles.
After connecting to the STM32 development board, run the automatic demo:
```
python inference_demo.py
```
Run the interactive demo:
```
python visual_demo.py
```
Put the description file in the project directory and the gscv test dataset in ./data directory. Run the interactive demo with audio:
```
python visual_demo_audio.py
```

# Special Notation
* The RNN model is trained in the pytrorch directory from author's previous project. It is uploaded with this main repo
* The features used in this repo is also generated in the pytorch repo.
* To get the same reported performance, use the models in directory ./model_back